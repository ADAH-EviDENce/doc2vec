{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create snippets from GV documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook allows the total corpus of Getuigenverhalen to be divided into snippets of approimately a predefined length, whilst preserving the paragraph and/or sentence structure identified by the Newsreader pipeline. The user can choose to create snippets of the actual terms/text, or alternatively create snippets with the terms replaced by the lemmas identified by the NR pipeline. I.e. it takes the output from the NR pipeline and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KafNafParserPy import KafNafParser\n",
    "import os\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KafNaf extraction utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of term ids from naf file\n",
    "def get_KafNaf_term_ids_list(parser):\n",
    "\n",
    "    terms = parser.get_terms()\n",
    "    \n",
    "    term_id_list = []\n",
    "    term_span_id_list = []\n",
    "    \n",
    "    for term in terms:\n",
    "        term_id = term.get_id()\n",
    "        term_span_id = term.get_span_ids()\n",
    "        term_id_list.append(term_id)\n",
    "        term_span_id_list.append(term_span_id[0])\n",
    "        \n",
    "    return term_id_list, term_span_id_list\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of lemmas for each term\n",
    "def get_KafNaf_term_lemma_list(parser):\n",
    "\n",
    "    lemma_list = []\n",
    "\n",
    "    terms = parser.get_terms()\n",
    "    \n",
    "    for term in terms:\n",
    "        lemma = term.get_lemma()\n",
    "        lemma_list.append(lemma)\n",
    "        \n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get id of sentence containing term for each term\n",
    "def get_KafNaf_wf_sentence_ids_list(parser, term_span_id_list):\n",
    "\n",
    "    sentence_id_list = []\n",
    "    \n",
    "    for spanid in term_span_id_list :\n",
    "        sentence_id = parser.text_layer.get_wf(spanid).get_sent()\n",
    "        sentence_id_list.append(sentence_id)\n",
    "        \n",
    "    return sentence_id_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get id of paragraph containing term for each term\n",
    "def get_KafNaf_wf_paragraph_ids_list(parser, term_span_id_list):\n",
    "\n",
    "    paragraph_id_list = []\n",
    "    \n",
    "    for spanid in term_span_id_list :\n",
    "        paragraph_id = parser.text_layer.get_wf(spanid).get_para()\n",
    "        paragraph_id_list.append(paragraph_id)\n",
    "        \n",
    "    return paragraph_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text (word) for each term\n",
    "def get_KafNaf_wf_text_list(parser, term_span_id_list):\n",
    "\n",
    "    text_list = []\n",
    "    \n",
    "    for spanid in term_span_id_list :\n",
    "        text = parser.text_layer.get_wf(spanid).get_text()\n",
    "        text_list.append(text)\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve information from the naf file\n",
    "def get_KafNaf_info(file):\n",
    "\n",
    "    #instantiate parser\n",
    "    parser = KafNafParser(file)\n",
    "    \n",
    "    #get term an correwsponding span id for each term\n",
    "    term_id_list, term_span_id_list = get_KafNaf_term_ids_list(parser)\n",
    "    \n",
    "    #get sentence id for each term\n",
    "    term_sentence_id_list = get_KafNaf_wf_sentence_ids_list(parser, term_span_id_list)\n",
    "    \n",
    "    #get paragraph id for each term\n",
    "    term_paragraph_id_list = get_KafNaf_wf_paragraph_ids_list(parser, term_span_id_list)\n",
    "    \n",
    "    #get text for each term\n",
    "    term_text_list = get_KafNaf_wf_text_list(parser, term_span_id_list)\n",
    "    \n",
    "    #get lemmas for each term\n",
    "    term_lemma_list = get_KafNaf_term_lemma_list(parser)\n",
    "    \n",
    "    term_info_dict ={'id':term_id_list,'span_id':term_span_id_list,'sentence':term_sentence_id_list,'paragraph':term_paragraph_id_list,'text':term_text_list,'lemma':term_lemma_list}\n",
    "    \n",
    "    return term_info_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunk(data,ids,uids,uid_start,target_size):\n",
    "\n",
    "    data_chunk = np.array([]) \n",
    "    current_size = 0\n",
    "    index = np.array(np.where(uids == uid_start)).flatten()\n",
    "    echo_uid_start = uids[np.copy(index)]\n",
    "    while current_size <= target_size:\n",
    "        sub_chunk_uid = uids[index]\n",
    "        data_sub_chunk = data[np.where(ids == sub_chunk_uid)]\n",
    "        data_chunk = np.append(data_chunk,data_sub_chunk)\n",
    "        current_size += len(data_sub_chunk)\n",
    "        index_completed = np.copy(index)\n",
    "        index += 1\n",
    "        if index == len(uids):\n",
    "            break\n",
    "\n",
    "        \n",
    "        \n",
    "    data_chunk_snippet = ' '.join(data_chunk)\n",
    "    uid_end = uids[index_completed]\n",
    "    \n",
    "    if index == len(uids):\n",
    "        uid_next = uid_end\n",
    "    else:\n",
    "        uid_next = uids[index]\n",
    "            \n",
    "    \n",
    "    return data_chunk_snippet, echo_uid_start, uid_end, uid_next, current_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks of text of approximately the target size making sure to not split paragraphs\n",
    "def create_chunk_snippets(data_key, id_key, term_info_dict, target_size,filename):\n",
    "    \n",
    "    \n",
    "    chunk_snippets = []\n",
    "    chunk_snippets_start_id = []\n",
    "    chunk_snippets_end_id = []\n",
    "    chunk_snippets_size =[]\n",
    "    \n",
    "    #cast to array type\n",
    "    data_arr = np.array(term_info_dict[data_key])\n",
    "    id_arr = np.array(term_info_dict[id_key])\n",
    "    \n",
    "    #get list of unique paragraph ids\n",
    "    uniq_id_arr = np.array(list(dict.fromkeys(id_arr)))\n",
    "    \n",
    "    if 'NIOD' in filename:\n",
    "        if id_key == 'paragraph' :\n",
    "            start_id = '57'\n",
    "        if id_key == 'sentence' :\n",
    "            start_id = '57'\n",
    "    else :\n",
    "        start_id = uniq_id_arr[0]\n",
    "        \n",
    "    done_chunking = False\n",
    "    \n",
    "    while done_chunking == False:\n",
    "        \n",
    "        chunk_snippet, echo_start_id, end_id, next_id, snippet_size = make_chunk(data_arr,id_arr, uniq_id_arr, start_id,target_size)\n",
    "\n",
    "        chunk_snippets_start_id.append(echo_start_id)\n",
    "        chunk_snippets_end_id.append(end_id)\n",
    "        chunk_snippets_size.append(snippet_size)\n",
    "        chunk_snippets.append(chunk_snippet)\n",
    "        \n",
    "        if end_id == next_id:\n",
    "            done_chunking = True\n",
    "            \n",
    "            \"\"\"merge small final chunk with preceding chunk\"\"\"\n",
    "            if chunk_snippets_size[-1] <= target_size//2 :\n",
    "                \n",
    "                final_chunk = chunk_snippets[-2]+' '+chunk_snippets[-1]\n",
    "                chunk_snippets[-2] = final_chunk\n",
    "                chunk_snippets = chunk_snippets[0:-1]\n",
    "                #---#\n",
    "                chunk_snippets_start_id = chunk_snippets_start_id[0:-1]\n",
    "                #---#\n",
    "                chunk_snippets_end_id[-2]=chunk_snippets_end_id[-1]\n",
    "                chunk_snippets_end_id = chunk_snippets_end_id[0:-1]\n",
    "                #---#\n",
    "                chunk_snippets_size[-2] = len(final_chunk)\n",
    "                chunk_snippets_size = chunk_snippets_size[0:-1]\n",
    "            \n",
    "        \n",
    "        start_id = next_id\n",
    "        \n",
    "    chunk_dict = {'chunk':chunk_snippets,'start_id':chunk_snippets_start_id,'end_id':chunk_snippets_end_id,'size':chunk_snippets_size,'data_type':data_key,'preserve_type':id_key,'target_size':target_size} \n",
    "    \n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chunk_snippets(filename,chunk_dict):\n",
    "    #\n",
    "    \n",
    "    for idx,chunk in enumerate(chunk_dict['chunk']):\n",
    "        chunk_file = filename+'_'+str(chunk_dict['target_size'])+'_'+chunk_dict['preserve_type']+'_'+str(chunk_dict['start_id'][idx][0])+'-'+str(chunk_dict['end_id'][idx][0])+'_'+chunk_dict['data_type']+'.txt'\n",
    "        \n",
    "        with open(chunk_file,'w') as out_file:\n",
    "            out_file.write(chunk)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic Inputs\n",
    "   text snippets preserving paragraph structure\n",
    "\"\"\"\n",
    "\n",
    "#desired approximate length of snippets (in terms)\n",
    "target_size = 150\n",
    "#desired data type (text/term or lemmas)\n",
    "data_type = 'text'\n",
    "#desired conserved entity (sentence or paragraph)\n",
    "preserve_type = 'paragraph'\n",
    "\n",
    "\"\"\"Directory structure\"\"\"\n",
    "inputdir='/Users/eslt0101/Data/eScience/EviDENce/Data/NR-Teksts/EviDENce_NR_output_clean/'\n",
    "\n",
    "outputdir=inputdir+'TargetSize'+str(target_size)+'/'+data_type+'_preserve_'+preserve_type+'/'\n",
    "\n",
    "\n",
    "\"\"\" make output directory\"\"\"\n",
    "os.makedirs(outputdir,exist_ok=True)\n",
    "\n",
    "\"\"\" change to input directory \"\"\"\n",
    "os.chdir(inputdir)\n",
    "\n",
    "directory = os.fsencode(inputdir)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    # for every file in input directory\n",
    "    full_file_name = os.fsdecode(file)\n",
    "    print(full_file_name)\n",
    "    if full_file_name.endswith('.naf'):\n",
    "        # create separate directories for the file (NB in output directory) \n",
    "        filename, file_extension = os.path.splitext(full_file_name)\n",
    "        dirname = os.path.join(outputdir,filename)\n",
    "        os.makedirs(dirname,exist_ok=True)                      \n",
    "        # create chunks for the file\n",
    "        file_term_info_dict = get_KafNaf_info(file)\n",
    "        if file_term_info_dict['id'] != []:\n",
    "            file_chunk_dict = create_chunk_snippets(data_type,preserve_type,file_term_info_dict,target_size,full_file_name)\n",
    "        # write chunks for file\n",
    "            os.chdir(dirname)\n",
    "            write_chunk_snippets(filename,file_chunk_dict)\n",
    "            os.chdir(inputdir)\n",
    "                           \n",
    "                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic Inputs\n",
    "   lemma snippets preserving paragraph structure\n",
    "\"\"\"\n",
    "\n",
    "#desired approximate length of snippets (in terms)\n",
    "target_size = 150\n",
    "#desired data type (text/term or lemmas)\n",
    "data_type = 'lemma'\n",
    "#desired conserved entity (sentence or paragraph)\n",
    "preserve_type = 'paragraph'\n",
    "\n",
    "\"\"\"Directory structure\"\"\"\n",
    "inputdir='/Users/eslt0101/Data/eScience/EviDENce/Data/NR-Teksts/EviDENce_NR_output_clean/'\n",
    "\n",
    "outputdir=inputdir+'TargetSize'+str(target_size)+'/'+data_type+'_preserve_'+preserve_type+'/'\n",
    "\n",
    "\n",
    "\"\"\" make output directory\"\"\"\n",
    "os.makedirs(outputdir,exist_ok=True)\n",
    "\n",
    "\"\"\" change to input directory \"\"\"\n",
    "os.chdir(inputdir)\n",
    "\n",
    "directory = os.fsencode(inputdir)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    # for every file in input directory\n",
    "    full_file_name = os.fsdecode(file)\n",
    "    print(full_file_name)\n",
    "    if full_file_name.endswith('.naf'):\n",
    "        # create separate directories for the file (NB in output directory) \n",
    "        filename, file_extension = os.path.splitext(full_file_name)\n",
    "        dirname = os.path.join(outputdir,filename)\n",
    "        os.makedirs(dirname,exist_ok=True)                      \n",
    "        # create chunks for the file\n",
    "        file_term_info_dict = get_KafNaf_info(file)\n",
    "        if file_term_info_dict['id'] != []:\n",
    "            file_chunk_dict = create_chunk_snippets(data_type,preserve_type,file_term_info_dict,target_size,full_file_name)\n",
    "        # write chunks for file\n",
    "            os.chdir(dirname)\n",
    "            write_chunk_snippets(filename,file_chunk_dict)\n",
    "            os.chdir(inputdir)\n",
    "                           \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
