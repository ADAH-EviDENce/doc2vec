{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from smart_open import smart_open\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "usecores=np.int(3*cores/4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and process stopwords. List has been gatherd from various sources and expanded to include (some) utterances such as uhm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/eslt0101/Projects/EviDENce/ML/stopwords-nl-stopwords-iso_expanded.txt','r') as stopwords_in:\n",
    "    dutch_stopwords =stopwords_in.readlines()\n",
    "    for i,sw in enumerate(dutch_stopwords):\n",
    "        dutch_stopwords[i] = sw.replace('\\n','')\n",
    "        \n",
    "    dutch_stopwords = gensim.utils.simple_preprocess(' '.join(dutch_stopwords),deacc=True)\n",
    "    #print(dutch_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_token_file,labeled=False):\n",
    "    with smart_open(corpus_token_file,'r') as tf:\n",
    "        for i,text_line in enumerate(tf):\n",
    "            if labeled :\n",
    "                #yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(text_line), [fn_line.rstrip()])\n",
    "                yield gensim.models.doc2vec.TaggedDocument([token for token in gensim.utils.simple_preprocess(text_line,deacc=True) if token not in dutch_stopwords], [i])\n",
    "            else:\n",
    "                yield [token for token in gensim.utils.simple_preprocess(text_line,deacc=True) if token not in dutch_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (below) reads in the corpus in unprocessed fashin allowing inspection for relevance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_lookup(corpus_filenames_file, corpus_token_file) :\n",
    "    with smart_open(corpus_filenames_file, 'r') as fnf, smart_open(corpus_token_file,'r') as tf:\n",
    "        i=0\n",
    "        for (fn_line,tf_line) in zip(fnf,tf):\n",
    "            yield ([i],[fn_line.rstrip()],[tf_line])\n",
    "            i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = '/Users/eslt0101/Data/eScience/EviDENce/Data/NR-Teksts/EviDENce_NR_output/TargetSize150'\n",
    "#corpus_text_file = 'test_GV_corpus_terms_para.txt'\n",
    "#corpus_filenames = 'test_GV_corpus_terms_para_filenames.txt'\n",
    "corpus_token_file = 'GV_corpus_terms_para_150.txt'\n",
    "corpus_text_file = 'GV_corpus_terms_para_150.txt'\n",
    "corpus_filenames = 'GV_corpus_terms_para_150_filenames.txt'\n",
    "full_ctokf = os.path.join(corpus_dir,corpus_token_file)\n",
    "full_ctf = os.path.join(corpus_dir,corpus_text_file)\n",
    "full_cff = os.path.join(corpus_dir,corpus_filenames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = list(read_corpus(full_ctokf,labeled=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_lookup = list(read_corpus_lookup(full_cff,full_ctokf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=50, negative=15, hs=0, min_count=4, epochs=30, workers=usecores)\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=30)#, workers=usecores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(corp, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load or save as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/Users/eslt0101/Projects/EviDENce/ML/model_default_v50_mc2_e30_freeze.d2v')\n",
    "model = gensim.utils.SaveLoad.load('/Users/eslt0101/Projects/EviDENce/ML/model_default_v50_mc2_e30_freeze.d2v')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an initial test of retrieval on the full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "first_rank =[]\n",
    "second_ranks = []\n",
    "least_ranks =[]\n",
    "doc_id =0\n",
    "#for i in range(len(corp)):\n",
    "for i in range(100):\n",
    "    inferred_vector = model.infer_vector(corp[i].words)\n",
    "    inferred_vector_tag = corp[i].tags\n",
    "    print('actual snippet')\n",
    "    print(corp[i].words)\n",
    "    print(corp[i].tags)\n",
    "    print(\"-------------\")\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [tag for tag,sim in sims].index(inferred_vector_tag)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])\n",
    "    least_ranks.append(sims[-1])\n",
    "    print(corp_lookup[sims[0][0]])\n",
    "    print(\"++++\")\n",
    "    print(rank)\n",
    "    print(corp_lookup[inferred_vector_tag[0]])\n",
    "    #print(sims[1])\n",
    "    #print(sims[2])\n",
    "    #print(sims[-1])\n",
    "    print('=======================')\n",
    "    print\n",
    "#sims[0]\n",
    "    #rank = [tag for tag, sim in sims].index(inferred_vector_tag)\n",
    "    #ranks.append(rank)\n",
    "    #second_ranks.append(sims[1])\n",
    "    #least_ranks.append[sims[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fragments 1 & 2 are self-composed 3 is an actual fragmnent from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term = 'hij sloeg er op dat het kapot ging en schoot met zijn geweer. het was verschrikkelijk om aan te zien. ik verstopte me, maar kon het geluid van de explosies niet uit mijn ooren krijgen. Toen het over was ben ik zo snel ik kon gaan rennen.'\n",
    "test_term_2 = 'Toen waren wij dus in Auschwitz. En toen? Nou, direkt naar een of ander gebouw toe. Uitkleden en de gevangenis kleding aan doen. Ik zag ook meteen al hoe een jongetje van misschien 4 of 5 van zijn moeder weg getrokken werd door een officier van de SS. En maar huilen en gillen.'#' Totdat een schot viel en alles stil was.'\n",
    "test_term_3 = \"Een razzia . Dus die ... . 'Ja , wat doen we ? ' . En we lopen over het dak , en toen hoorden we vanaf de graansilo , daar stond een luchtafweergeschut , en daar kregen we vuur van . Toen vlogen de kogels om onze oren . Toen zij we als de sodomieter via de schoorsteen , in dekking naar Dordtselaan 98 gevlucht . En daar kwamen we en ja , wat doen we ? Ja , wij gaan niet naar beneden toe . Dus wij gingen niet naar beneden toe . Wij gingen perse niet naar beneden .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process test fragments as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs_test_term = [token for token in gensim.utils.simple_preprocess(test_term,deacc=True) if token not in dutch_stopwords]\n",
    "procs_test_term_2 = [token for token in gensim.utils.simple_preprocess(test_term_2,deacc=True) if token not in dutch_stopwords]\n",
    "procs_test_term_3 = [token for token in gensim.utils.simple_preprocess(test_term_3,deacc=True) if token not in dutch_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infer vectors for test fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_vector_test_term = model.infer_vector(procs_test_term)\n",
    "inf_vector_test_term_2 = model.infer_vector(procs_test_term_2)\n",
    "inf_vector_test_term_3 = model.infer_vector(procs_test_term_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is unstable. infer vectors as median of 100 inferral instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infarr1 = []\n",
    "infarr1_cosvar = []\n",
    "infarr2 = []\n",
    "infarr3 = []\n",
    "\n",
    "for i in range(100):\n",
    "    inf1 = model.infer_vector(procs_test_term)\n",
    "    inf1_2 = model.infer_vector(procs_test_term)\n",
    "    inf2 = model.infer_vector(procs_test_term_2)\n",
    "    inf3 = model.infer_vector(procs_test_term_3)\n",
    "    cosvar = 1 - sp.spatial.distance.cosine(inf1,inf1_2)\n",
    "    infarr1.append(inf1)\n",
    "    infarr2.append(inf2)\n",
    "    infarr3.append(inf3)\n",
    "    infarr1_cosvar.append(cosvar)\n",
    "    \n",
    "med_infarr1 = np.median(np.array(infarr1),axis=0) #.mean(axis=0)\n",
    "mean_infarr1 = np.array(infarr1).mean(axis=0)\n",
    "std_infarr1 = np.array(infarr1).std(axis=0)\n",
    "mean_infarr1_cosvar = np.array(infarr1_cosvar).mean(axis=0)\n",
    "med_infarr2 = np.median(np.array(infarr2),axis=0) #.mean(axis=0)\n",
    "mean_infarr2 = np.array(infarr2).mean(axis=0)\n",
    "std_infarr2 = np.array(infarr2).std(axis=0)\n",
    "mean_infarr3 = np.array(infarr3).mean(axis=0)\n",
    "med_infarr3 = np.median(np.array(infarr3),axis=0)#.mean(axis=0)\n",
    "std_infarr3 = np.array(infarr3).std(axis=0)\n",
    "\n",
    "#print(mean_infarr1)\n",
    "#print(model.infer_vector(procs_test_term,steps=100))\n",
    "print(med_infarr1)\n",
    "print(std_infarr1)\n",
    "print(mean_infarr1_cosvar)\n",
    "print('----')\n",
    "print(mean_infarr3)\n",
    "print(med_infarr3)\n",
    "print(std_infarr3)\n",
    "print('++++')\n",
    "print(inf_vector_test_term_3)\n",
    "print(inf_vector_test_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar document vectors as initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims = model.docvecs.most_similar([inf_vector_test_term], topn=len(model.docvecs))\n",
    "#sims_2 = model.docvecs.most_similar([inf_vector_test_term_2], topn=len(model.docvecs))\n",
    "#sims_3 = model.docvecs.most_similar([inf_vector_test_term_3], topn=len(model.docvecs))\n",
    "\n",
    "sims = model.docvecs.most_similar([med_infarr1], topn=len(model.docvecs))\n",
    "sims_2 = model.docvecs.most_similar([med_infarr2], topn=len(model.docvecs))\n",
    "sims_3 = model.docvecs.most_similar([med_infarr3], topn=len(model.docvecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corp[0].words\n",
    "#test = model.docvecs.similarity(corp[0].words,corp[0].words)\n",
    "#print(test)\n",
    "#sp.spatial.distance.cosine(test, model.docvecs[0])\n",
    "#ivtt_12 = np.copy(inf_vector_test_term)\n",
    "#type(inf_vector_test_term)\n",
    "#model.save('/Users/eslt0101/Projects/EviDENce/ML/model_default_v50_mc2_e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200) :\n",
    "    print(sims[i])\n",
    "    if i == 19 :\n",
    "        print('===================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.docvecs.similarity_unseen_docs(model,corp[0].words,corp[0].words))\n",
    "#v1 = model.infer_vector(corp[0].words)\n",
    "#v2 = model.infer_vector(corp[0].words)\n",
    "#print(np.sqrt(1. - (sp.spatial.distance.cosine(v1,v2))**2))\n",
    "#print(1. - (sp.spatial.distance.cosine(v1,v2)))\n",
    "#sp.spatial.distance.cosine(mean_infarr1,model.docvecs[13946]) + 0.764286994934082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    id = sims[i][0]\n",
    "    print(corp_lookup[id])\n",
    "    print('+++++++++++++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reinfer corpus vectores on fully trained and froven model. these will be used for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%time reinf_corp_vec = [ model.infer_vector(c.words) for c in corp]\n",
    "\n",
    "reinf_corp_vec_med = np.ndarray((100,len(corp),50))\n",
    "i=0\n",
    "\n",
    "for j in range(100):\n",
    "    for i in range(len(corp)):\n",
    "        c = corp[i]\n",
    "        reinf_corp_vec_med[j][i][:] = model.infer_vector(c.words)\n",
    "        i+=1\n",
    "    j+=1\n",
    "\n",
    "\n",
    "    \n",
    "#print(aa)```````````````````````````````````````````````````````\n",
    "print('=========')\n",
    "print(np.median(reinf_corp_vec_med,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinf_corp_medvec = np.median(reinf_corp_vec_med,axis=0)\n",
    "print(reinf_corp_medvec)\n",
    "#print(np.median(reinf_corp_vec_med,axis=0).shape  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(reinf_corp_vec)\n",
    "#np.save('reinferred_corpus_terms_para_150_medvec.npy',reinf_corp_medvec)\n",
    "\n",
    "reinf_corp_medvec= np.load('/Users/eslt0101/Projects/EviDENce/ML/reinferred_corpus_terms_para_150_medvec.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cosine distance function to use with set of reinferred vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist = np.array([1. - sp.spatial.distance.cosine(med_infarr1,rinfc) for rinfc in reinf_corp_medvec])\n",
    "\n",
    "#cosdist.max()\n",
    "b=np.argsort(cosdist)[::-1]\n",
    "print(b[0:25])\n",
    "print(cosdist[b[0:25]])\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_3 = np.array([1. - sp.spatial.distance.cosine(med_infarr3,rinfc) for rinfc in reinf_corp_medvec])\n",
    "\n",
    "#cosdist.max()\n",
    "cc=np.argsort(cosdist_3)[::-1]\n",
    "print(cc[0:25])\n",
    "print(cosdist_3[cc[0:25]])\n",
    "len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant(idlist,corp_lookup):\n",
    "    selected= []\n",
    "    for j in range(len(idlist)):\n",
    "        jj = idlist[j]\n",
    "        print('---------Snippet '+str(jj)+'---------')\n",
    "        print(corp_lookup[jj])\n",
    "        print('------------')\n",
    "        print('Is this snippet relevant? [y/n]')\n",
    "        rel = input()\n",
    "        if rel == 'y' :\n",
    "            selected.append(jj)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The 25 most similar snippets will be displayed.')\n",
    "print(' Please select those you deem relevant. Are you ready to continue? [please select Yes/No]')\n",
    "answer = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant3=[]\n",
    "if answer == 'Yes':\n",
    "    relevant3 = select_relevant(cc[0:24],corp_lookup)\n",
    "    #for i in range(25):\n",
    "    #    ii = b[i]\n",
    "    #    print('---------Snippet '+str(ii)+'---------')\n",
    "    #    print(corp_lookup[ii])\n",
    "    #    print('------------')\n",
    "    #    print('Is this snippet relevant? [y/n]')\n",
    "    #    rel = input()\n",
    "    #    if rel == 'y' :\n",
    "    #        relevant.append(ii)\n",
    "    #    else :\n",
    "    #        pass\n",
    "else :\n",
    "    print('No relevant snippets have been selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below has been run several times in an interactive fashion to select and manually iterate over the selection process. This is very muchg still in a bleeding edge development state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corp_lookup))\n",
    "ph =(((np.random.rand(25)*27065).round()))\n",
    "print(ph)\n",
    "phi = [int(pp) for pp in ph]\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantrand=[]\n",
    "if answer == 'Yes':\n",
    "    \n",
    "    \n",
    "    \n",
    "    relevantrand = select_relevant(phi,corp_lookup)\n",
    "    #for i in range(25):\n",
    "    #    ii = b[i]\n",
    "    #    print('---------Snippet '+str(ii)+'---------')\n",
    "    #    print(corp_lookup[ii])\n",
    "    #    print('------------')\n",
    "    #    print('Is this snippet relevant? [y/n]')\n",
    "    #    rel = input()\n",
    "    #    if rel == 'y' :\n",
    "    #        relevant.append(ii)\n",
    "    #    else :\n",
    "    #        pass\n",
    "else :\n",
    "    print('No relevant snippets have been selected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevantrand\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_rel1 = np.array([1. - sp.spatial.distance.cosine(reinf_corp_medvec[20106],rinfc) for rinfc in reinf_corp_medvec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=np.argsort(cosdist_rel1)[::-1]\n",
    "print(b2[0:25])\n",
    "print(cosdist_rel1[b2[0:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant2=select_relevant(b2[0:25],corp_lookup)\n",
    "\n",
    "#for j in range(25):\n",
    "#    jj = b2[j]\n",
    "#    print\n",
    "#    print(corp_lookup[jj])\n",
    "#    print('-------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant2 = [20106, 10415, 13946, 1263, 24390, 18340, 17276, 24378, 2, 4207, 20153, 17197, 25237, 5441, 8439, 26618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reit_r2 = reinf_corp_medvec[relevant2].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_reit_r2 = np.array([1. - sp.spatial.distance.cosine(reit_r2,rinfc) for rinfc in reinf_corp_medvec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=np.argsort(cosdist_reit_r2)[::-1]\n",
    "print(r2[0:25])\n",
    "print(cosdist_reit_r2[r2[0:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_r2=select_relevant(r2[0:25],corp_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reit_r3 = reinf_corp_medvec[relevant_r2].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_reit_r3 = np.array([1. - sp.spatial.distance.cosine(reit_r3,rinfc) for rinfc in reinf_corp_medvec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3=np.argsort(cosdist_reit_r3)[::-1]\n",
    "print(r3[0:25])\n",
    "print(cosdist_reit_r3[r3[0:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_r3=select_relevant(r3[0:25],corp_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant_r3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
