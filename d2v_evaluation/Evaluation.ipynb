{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from smart_open import smart_open\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To rebuild ground truth corpus from scratch (requires use of d2v scripts on repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruthFile='/Users/wrvhage/Research/EviDENce/GroundTruth/Manual_annotation_TOTAAL_door_Jeroen_en_Susan.xlsx'\n",
    "groundTruthFileSheet='Resultaat Manuele Annotatie'\n",
    "groundTruth = pd.read_excel(groundTruthFile, sheet_name=groundTruthFileSheet)\n",
    "\n",
    "\n",
    "#groundTruth['Titel']\n",
    "\n",
    "\n",
    "#type(groundTruth)\n",
    "\n",
    "groundTruth.drop_duplicates(['Titel'],keep='first', inplace=True)\n",
    "groundTruth.reset_index(drop=True,inplace=True)\n",
    "#groundTruth['Titel'][121]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsFile = '/Users/wrvhage/Research/EviDENce/ML/stopwords-nl-stopwords-iso_expanded_gensim_processed.txt'\n",
    "corpus_dir = '/Users/wrvhage/Research/EviDENce/Data/NR-Teksts/EviDENce_NR_output_clean/TargetSize150'\n",
    "corpus_token_file = 'GV_corpus_text_para_clean.txt'\n",
    "corpus_text_file = 'GV_corpus_text_para_clean.txt'\n",
    "corpus_filenames_file = 'GV_corpus_text_para_clean_filenames.txt'\n",
    "\n",
    "full_ctokf = os.path.join(corpus_dir,corpus_token_file)\n",
    "full_ctf = os.path.join(corpus_dir,corpus_text_file)\n",
    "full_cff = os.path.join(corpus_dir,corpus_filenames_file) \n",
    "\n",
    "with open(stopwordsFile) as f:\n",
    "    dutch_stopwords = f.readlines()\n",
    "    \n",
    "\n",
    "dutch_stopwords = [w.rstrip() for  w in dutch_stopwords]\n",
    "\n",
    "\n",
    "def read_corpus(corpus_token_file,labeled=False):\n",
    "    with smart_open(corpus_token_file,'r') as tf:\n",
    "        for i,text_line in enumerate(tf):\n",
    "            if labeled :\n",
    "                #yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(text_line), [fn_line.rstrip()])\n",
    "                yield gensim.models.doc2vec.TaggedDocument([token for token in gensim.utils.simple_preprocess(text_line,deacc=True) if token not in dutch_stopwords], [i])\n",
    "            else:\n",
    "                yield [token for token in gensim.utils.simple_preprocess(text_line,deacc=True) if token not in dutch_stopwords]\n",
    "\n",
    "def read_corpus_lookup(corpus_filenames_file, corpus_token_file) :\n",
    "    with smart_open(corpus_filenames_file, 'r') as fnf, smart_open(corpus_token_file,'r') as tf:\n",
    "        i=0\n",
    "        for (fn_line,tf_line) in zip(fnf,tf):\n",
    "            yield ([i],[fn_line.rstrip()],[tf_line])\n",
    "            i+=1\n",
    "\n",
    "\n",
    "corp = list(read_corpus(full_ctokf,labeled=True))\n",
    "\n",
    "\n",
    "corp_lookup = list(read_corpus_lookup(full_cff,full_ctokf))\n",
    "\n",
    "reinf_corp_medvec= np.load('/Users/wrvhage/Research/EviDENce/ML/reinferred_corpus_terms_para_150_medvec_clean.npy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(full_cff,'r') as fnf:\n",
    "    corpFileNames=fnf.readlines()\n",
    "\n",
    "corpFileNames = [fn.rstrip().split('.')[0] for fn in corpFileNames ]\n",
    "#corpFileNames = [fn.rstrip() for fn in corpFileNames ]\n",
    "\n",
    "\n",
    "groundTruth_inCorpusIds =[]\n",
    "for j in range(len(groundTruth)):\n",
    "    #print(j, groundTruth['Titel'][j])\n",
    "    gtTitel = groundTruth['Titel'][j]\n",
    "    corpusIndex = [i for i,ent in enumerate(corpFileNames) if ent == gtTitel]\n",
    "    groundTruth_inCorpusIds.append(corpusIndex[0])\n",
    "\n",
    "\n",
    "\n",
    "#groundTruth_inCorpusIds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gt_corp_lookup = [corp_lookup[i] for i in groundTruth_inCorpusIds]\n",
    "\n",
    "\n",
    "\n",
    "gt_corp=[corp[i] for i in groundTruth_inCorpusIds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_corp_reinf_medvec=reinf_corp_medvec[groundTruth_inCorpusIds]\n",
    "\n",
    "\n",
    "### Append reinferred vectors to dataframe\n",
    "\n",
    "series_reinfVec =pd.Series(np.array([]))\n",
    "for i in range(len(gt_corp_reinf_medvec)):\n",
    "    series_reinfVec[i] = gt_corp_reinf_medvec[i]\n",
    "    \n",
    "\n",
    "\n",
    "groundTruthEx = groundTruth\n",
    "\n",
    "\n",
    "\n",
    "groundTruthEx.insert(10,'reinfVec',series_reinfVec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To import ground truth data set as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruthEx = pd.read_pickle('/Users/wrvhage/Research/EviDENce/doc2vec/d2v_evaluation/groundTruthEx_pd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruthEx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_pass(inVec, corpDF, nReturn):\n",
    "    \"\"\"calculate cosine distance of query vector inVec to all corpus vectors\"\"\"\n",
    "    cosd_vec_corp = cosdistance_corp(inVec,corpDF['reinfVec'])\n",
    "    \n",
    "    \"\"\"rank cosine distances in descending order\"\"\"\n",
    "    ranked_retrieval = np.argsort(cosd_vec_corp)[::-1]\n",
    "    \n",
    "    \"get cosine distances in ranked order\"\n",
    "    ret_cosd = cosd_vec_corp[ranked_retrieval]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" return top nReturn instances\"\"\"\n",
    "    ranking = corpDF.iloc[ranked_retrieval,[1,9,10]].copy()\n",
    "    ranking['cosine_distance']= ret_cosd\n",
    "    \n",
    "\n",
    "    \n",
    "    selected = ranking.iloc[0:nReturn].copy()\n",
    "    \"\"\" create pandas series of selected ranked cosine distances\"\"\"\n",
    "    #ret_cosd = pd.Series(ranked_cosd[0:nReturn])\n",
    "    \n",
    "    \"\"\"insert cosine distance into returned ranking\"\"\"\n",
    "    \n",
    "    \n",
    "    return selected\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosdistance(v1,v2):\n",
    "    d = np.array([1. - spatial.distance.cosine(v1,v2)])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosdistance_corp(vec,corpus_vectors):\n",
    "    cosd_vec_corp =[]\n",
    "    for i in range (len(corpus_vectors)):\n",
    "        cd = cosdistance(vec,corpus_vectors[i])\n",
    "        cosd_vec_corp.append(cd)\n",
    "    cosd_vec_corp = np.array(cosd_vec_corp).flatten()\n",
    "    return cosd_vec_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PR_LR(retrievedDF):\n",
    "    P_LR = len(np.array(np.where(retrievedDF['EINDOORDEEL_janee'] == 'ja')).flatten())/len(retrievedDF['EINDOORDEEL_janee'])\n",
    "    R_LR = P_LR\n",
    "    return P_LR, R_LR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(corpDF,lenRetrieve):\n",
    "\n",
    "    precisionLR =[]\n",
    "    recallLR =[]\n",
    "    queries =[]\n",
    "    \n",
    "    loc_yes = np.array(np.where(corpDF['EINDOORDEEL_janee'] == 'ja')).flatten()\n",
    "    \n",
    "    for i in range(len(loc_yes)):\n",
    "        queryVec = corpDF['reinfVec'][loc_yes[i]]\n",
    "        queryTitle = corpDF['Titel'][loc_yes[i]]\n",
    "        \n",
    "        incorp = corpDF.copy()\n",
    "        incorp.drop([loc_yes[i]],inplace=True)\n",
    "        incorp = incorp.reset_index(drop=True)\n",
    "        \n",
    "        retrieved = retrieval_pass(queryVec,incorp,lenRetrieve)\n",
    "        \n",
    "        retrievedout = retrieved.drop(columns=['reinfVec'])#.to_csv(queryTitle+'_'+str(lenRetrieve)+'.csv',index=False)\n",
    "        retrievedout.to_csv('/Users/wrvhage/Research/EviDENce/ML/evaluation_data/single_pass_retrieval/'+queryTitle+'_'+str(lenRetrieve)+'.csv',index=False)\n",
    "        \n",
    "        pLR, rLR = calculate_PR_LR(retrieved)\n",
    "        \n",
    "        precisionLR.append(pLR)\n",
    "        recallLR.append(rLR)\n",
    "        queries.append(queryTitle)\n",
    "        \n",
    "        \n",
    "    return precisionLR, recallLR, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_precisionAtR, sp_recallAtR, sp_queryTitle = run_evaluation(groundTruthEx,182)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'Title': sp_queryTitle, 'PR_at_R': sp_precisionAtR }\n",
    "results = pd.DataFrame(Data,columns=['Title','PR_at_R'])\n",
    "results.sort_values(by='Title',inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('/Users/wrvhage/Research/EviDENce/ML/evaluation_data/single_pass_retrieval/evaluation_at_R.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_meanPaR = np.array(sp_precisionAtR).mean()\n",
    "sp_stdPaR = np.array(sp_precisionAtR).std()\n",
    "sp_maxPaR = np.array(sp_precisionAtR).max()\n",
    "sp_minPaR = np.array(sp_precisionAtR).min()\n",
    "sp_NAboveBaseline = len(np.array(np.where(np.array(sp_precisionAtR) > 0.1848)).flatten())\n",
    "sp_FracAboveBaseline = sp_NAboveBaseline/183\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_meanPaR, sp_stdPaR, sp_maxPaR, sp_minPaR,sp_NAboveBaseline,sp_FracAboveBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "182/985 #random sample based expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldstandard_relevance_feedback(inVec, ranking, corpDF, k):\n",
    "    # assume user has judged the top-k documents for relevance, neutrality and irrelevance\n",
    "#    relevant_retrieved = np.array(np.where(ranking['EINDOORDEEL_janee'] == 'ja')).flatten()\n",
    "#    irrelevant_retrieved = np.array(np.where(ranking['EINDOORDEEL_janee'] == 'nee')).flatten() # assume no neutral in manual annotations\n",
    "\n",
    "#    print(type(relevant_retrieved))\n",
    "#    print(relevant_retrieved)\n",
    "    \n",
    "#    relrank = ranking.loc[relevant_retrieved,['reinfVec']]\n",
    "#    irrelrank = ranking.loc[irrelevant_retrieved,['reinfVec']]\n",
    "    \n",
    "    \n",
    "    return relevance_feedback_pass(inVec, \n",
    "                                   ranking[ranking['EINDOORDEEL_janee'] == 'ja']['reinfVec'],\n",
    "                                   ranking[ranking['EINDOORDEEL_janee'] == 'nee']['reinfVec'],\n",
    "                                   corpDF,\n",
    "                                   k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_feedback_pass(inVec, relevantVecs, irrelevantVecs, corpDF, k):\n",
    "    # Rocchio relevance feedback\n",
    "    relevantMeanVec = np.mean(relevantVecs, axis=0)\n",
    "    irrelevantMeanVec = np.mean(irrelevantVecs, axis=0)\n",
    "\n",
    "    a = 1\n",
    "    b = 0.8\n",
    "    c = 0.1\n",
    "    \n",
    "    updatedQueryVec = a * inVec + b * relevantMeanVec + c * irrelevantMeanVec\n",
    "    \n",
    "    return updatedQueryVec, retrieval_pass(updatedQueryVec, corpDF, 182)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feedback_evaluation(corpDF,lenRetrieve,lenSimUserFeedback):\n",
    "\n",
    "    precisionLR =[]\n",
    "    recallLR =[]\n",
    "    queries =[]\n",
    "    \n",
    "    loc_yes = np.array(np.where(corpDF['EINDOORDEEL_janee'] == 'ja')).flatten()\n",
    "    \n",
    "    for i in range(len(loc_yes)):\n",
    "        queryVec = corpDF['reinfVec'][loc_yes[i]]\n",
    "        queryTitle = corpDF['Titel'][loc_yes[i]]\n",
    "        \n",
    "        incorp = corpDF.copy()\n",
    "        incorp.drop([loc_yes[i]],inplace=True)\n",
    "        incorp = incorp.reset_index(drop=True)\n",
    "        \n",
    "        ranking = retrieval_pass(queryVec,incorp,lenSimUserFeedback)\n",
    "        updated_queryvec, retrieved = goldstandard_relevance_feedback(queryVec, ranking, incorp, lenSimUserFeedback)\n",
    "\n",
    "        retrievedout = retrieved.drop(columns=['reinfVec'])#.to_csv(queryTitle+'_'+str(lenRetrieve)+'.csv',index=False)\n",
    "        retrievedout.to_csv('/Users/wrvhage/Research/EviDENce/ML/evaluation_data/goldstandard_relevance_feedback_pass/'+queryTitle+'_'+str(lenRetrieve)+'.csv',index=False)\n",
    "        \n",
    "        pLR, rLR = calculate_PR_LR(retrieved)\n",
    "        \n",
    "        precisionLR.append(pLR)\n",
    "        recallLR.append(rLR)\n",
    "        queries.append(queryTitle)\n",
    "        \n",
    "        \n",
    "    return precisionLR, recallLR, queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_precisionAtR, rf_recallAtR, rf_queryTitle = run_feedback_evaluation(groundTruthEx,182,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'Title': rf_queryTitle, 'PR_at_R': rf_precisionAtR }\n",
    "results = pd.DataFrame(Data,columns=['Title','PR_at_R'])\n",
    "results.sort_values(by='Title',inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('/Users/wrvhage/Research/EviDENce/ML/evaluation_data/goldstandard_relevance_feedback_pass/evaluation_at_R.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_meanPaR = np.array(rf_precisionAtR).mean()\n",
    "rf_stdPaR = np.array(rf_precisionAtR).std()\n",
    "rf_maxPaR = np.array(rf_precisionAtR).max()\n",
    "rf_minPaR = np.array(rf_precisionAtR).min()\n",
    "rf_NAboveBaseline = len(np.array(np.where(np.array(rf_precisionAtR) > 0.1848)).flatten())\n",
    "rf_FracAboveBaseline = rf_NAboveBaseline/183\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_meanPaR, rf_stdPaR, rf_maxPaR, rf_minPaR, rf_NAboveBaseline, rf_FracAboveBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "182/985 # random sample expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.array(rf_precisionAtR) - np.array(sp_precisionAtR)) > 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
